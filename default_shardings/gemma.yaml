
# Sharding config for gemma
# "replicated" to signify "replicated".
# Integer signify axis to shard: 0 <= shard axis < rank

freqs_cis : -1 #  torch.complex64 (16384, 128)
layers.*.self_attn.o_proj.weight: 1
layers.*.self_attn.wq.weight : 0 # -1, 1] #  torch.float32 (2048, 2048)
layers.*.self_attn.wk.weight : 0 # -1, 1] #  torch.float32 (256, 2048)
layers.*.self_attn.wv.weight : 0 # -1, 1] #  torch.float32 (256, 2048)
layers.*.mlp.gate_proj.weight : 0 # -1, 1] #  torch.float32 (16384, 2048)
layers.*.mlp.gate_proj.bias : 0 # -1] #  torch.float32 (16384,)
layers.*.mlp.up_proj.weight : 0 # -1, 1] #  torch.float32 (16384, 2048)
layers.*.mlp.up_proj.bias : 0 # -1] #  torch.float32 (16384,)
layers.*.mlp.down_proj.weight : 1 # 1, -1] #  torch.float32 (2048, 16384)
layers.*.mlp.down_proj.bias : -1 #  torch.float32 (2048,)
layers.*.input_layernorm.weight : -1 #  torch.float32 (2048,)
layers.*.post_attention_layernorm.weight : -1 #  torch.float32 (2048,)
norm.weight : -1 #  torch.float32 (2048,)
embedder.weight : 1 # # 1, -1] #  torch.float32 (256000, 2048)
embedder.weight_scaler : 0 
layers.*.self_attn.o_proj.weight_scaler: 0
layers.*.self_attn.wq.weight_scaler : 0 
layers.*.self_attn.wk.weight_scaler : 0 
layers.*.self_attn.wv.weight_scaler : 0 
layers.*.mlp.gate_proj.weight_scaler : 0 
layers.*.mlp.up_proj.weight_scaler : 0 
layers.*.mlp.down_proj.weight_scaler : 0 
