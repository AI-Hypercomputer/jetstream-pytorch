
# Sharding config for gemma
# "replicated" to signify "replicated".
# Integer signify axis to shard: 0 <= shard axis < rank

freqs_cis : null #  torch.complex64 (16384, 128)
layers.*.self_attn.wo.weight : 1 # 1, -1] #  torch.float32 (2048, 2048)
layers.*.self_attn.wq.weight : 0 # -1, 1] #  torch.float32 (2048, 2048)
layers.*.self_attn.wk.weight : 0 # -1, 1] #  torch.float32 (256, 2048)
layers.*.self_attn.wv.weight : 0 # -1, 1] #  torch.float32 (256, 2048)
layers.*.mlp.gate_proj.weight : 0 # -1, 1] #  torch.float32 (16384, 2048)
layers.*.mlp.gate_proj.bias : 0 # -1] #  torch.float32 (16384,)
layers.*.mlp.up_proj.weight : 0 # -1, 1] #  torch.float32 (16384, 2048)
layers.*.mlp.up_proj.bias : 0 # -1] #  torch.float32 (16384,)
layers.*.mlp.down_proj.weight : 1 # 1, -1] #  torch.float32 (2048, 16384)
layers.*.mlp.down_proj.bias : null #  torch.float32 (2048,)
layers.*.input_layernorm.weight : null #  torch.float32 (2048,)
layers.*.post_attention_layernorm.weight : null #  torch.float32 (2048,)
norm.weight : null #  torch.float32 (2048,)
embedder.weight : 1 # # 1, -1] #  torch.float32 (256000, 2048)
