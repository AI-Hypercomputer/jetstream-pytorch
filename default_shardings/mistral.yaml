
# Sharding config for mixtral
# Sharding should either be an int between 0 and rank - 1
# signifying the axis to shard or -1 / null signifying replicated


freqs_cis : -1 #  torch.complex64 (2048, 64)
tok_embeddings.weight : 1 #  torch.float32 (vocab_size, 4096)
tok_embeddings.weight_scaler : 0 #  torch.bfloat16 (4096,)
layers.*.attention.wo.weight : 1 #  torch.int8 (4096, 4096)
layers.*.attention.wo.weight_scaler : 0 #  torch.bfloat16 (4096,)
layers.*.attention.wq.weight : 0 #  torch.int8 (4096, 4096)
layers.*.attention.wq.weight_scaler : 0 #  torch.bfloat16 (4096,)
layers.*.attention.wk.weight : 0 #  torch.int8 (4096, 4096)
layers.*.attention.wk.weight_scaler : 0 #  torch.bfloat16 (4096,)
layers.*.attention.wv.weight : 0 #  torch.int8 (4096, 4096)
layers.*.attention.wv.weight_scaler : 0 #  torch.bfloat16 (4096,)
layers.*.block_sparse_moe.gate.weight: 1
layers.*.block_sparse_moe.gate.weight_scaler: 0
layers.*.block_sparse_moe.cond_ffn.w1: 2
layers.*.block_sparse_moe.cond_ffn.w1_scaler: 0
layers.*.block_sparse_moe.cond_ffn.w2: 1 
layers.*.block_sparse_moe.cond_ffn.w2_scaler: 0
layers.*.block_sparse_moe.cond_ffn.w3: 2
layers.*.block_sparse_moe.cond_ffn.w3_scaler: 0
layers.*.ffn_norm.weight : -1 #  torch.float32 (4096,)
layers.*.attention_norm.weight : -1 #  torch.float32 (4096,)
norm.weight : -1 #  torch.float32 (4096,)
output.weight : 0 #  torch.float32 (vocab_size, 4096)
#output.weight_scaler : 0 #  torch.float32 (4096,)
