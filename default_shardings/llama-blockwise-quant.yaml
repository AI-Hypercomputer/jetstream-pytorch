
# Sharding config for llama-2 (With blockwise quantized linear layers)
# Sharding should either be an int between 0 and rank - 1
# signifying the axis to shard or -1 / null signifying replicated


freqs_cis : -1 #  torch.complex64 (2048, 64)
tok_embeddings.weight : -1 #  torch.int8 (32000, 4096)
tok_embeddings.weight_scaler : -1 #  torch.bfloat16 (4096,)
layers.*.attention.wo.weight : 0 #  torch.int8 (32, 128, 4096)
layers.*.attention.wo.weight_scaler : 0 #  torch.bfloat16 (32, 4096)
layers.*.attention.wq.weight : 2 #  torch.int8 (32, 128, 4096)
layers.*.attention.wq.weight_scaler : 1 #  torch.bfloat16 (32, 4096)
layers.*.attention.wk.weight : 2 #  torch.int8 (32, 128, 4096)
layers.*.attention.wk.weight_scaler : 1 #  torch.bfloat16 (32, 4096)
layers.*.attention.wv.weight : 2 #  torch.int8 (32, 128, 4096)
layers.*.attention.wv.weight_scaler : 1 #  torch.bfloat16 (32, 4096)
layers.*.feed_forward.w1.weight : 2 #  torch.int8 (32, 128, 11008)
layers.*.feed_forward.w1.weight_scaler : 1  # torch.bfloat16 (32, 11008)
layers.*.feed_forward.w2.weight : 0 #  torch.int8 (86, 128, 4096)
layers.*.feed_forward.w2.weight_scaler : 0 # torch.bfloat16 (86, 4096)
layers.*.feed_forward.w3.weight : 2 #  torch.int8 (32, 128, 11008)
layers.*.feed_forward.w3.weight_scaler : 1 # torch.bfloat16 (32, 11008)
layers.*.attention_norm.weight : -1 #  torch.float32 (4096,)
layers.*.ffn_norm.weight : -1 #  torch.float32 (4096,)
norm.weight : -1 #  torch.float32 (4096,)
output.weight : 2 #  torch.int8 (32, 128, 32000)
output.weight_scaler : 1 #  torch.float32 (32, 32000)
